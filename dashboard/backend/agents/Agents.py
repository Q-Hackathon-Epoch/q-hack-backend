from langchain_core.language_models import BaseChatModel
from langchain_core.output_parsers import StrOutputParser
from pydantic import BaseModel, Field
from typing import List, Dict
from langchain_core.prompts import ChatPromptTemplate

from  dashboard.backend.agents import Prompts

# Pydantic model
class PredicateItem(BaseModel):
    value: str

class PredicateSet(BaseModel):
    items: List[str]
class LLMVisibleResponse(BaseModel):
    answer: str = Field(description="The response generated by the language model.")
    # Represents List[List[str]]
    predicate_sets: List[PredicateSet] = Field(
        description="A list of the predicate sets that should be used."
    )
class LLMResponse(BaseModel):
    answer: str = Field(description="The response generated by the language model.")
    # Represents List[List[str]]
    predicates: List[str] = Field(
        description="A list of the predicates that should be used."
    )
# for openai also you can use this
example_schema = {
    "name": "llm_response",
    "schema": {
        "type": "object",
        "properties": {
            "answer": {
                "type": "string",
                "description": "The response generated by the language model."
            },
            "combination": {
                "type": "array",
                "description": "A list of the predicates from the best combination.",
                "items": {
                    "type": "string"
                }
            }
        },
        "required": [
            "answer",
            "combination"
        ],
        "additionalProperties": False
    },
    "strict": True
}

def get_chat_prompt_template(system, user):
    return ChatPromptTemplate.from_messages([("system", system), ("human", user)])


class Agent:
    def __init__(self, llm_model: BaseChatModel):
        self.llm = llm_model

    def get_answer(self, current_date: str, course_code: str, raw_text: str) -> Dict[str, List[str]]:
        prompt_template = get_chat_prompt_template(Prompts.system_prompt, )
        chain = prompt_template | self.llm.with_structured_output(example_schema)
        result = chain.invoke({'current_date': current_date,
                               'course_code': course_code,
                               'raw_text': raw_text})
        #answer = result['parsed']['should_include']
        return result
